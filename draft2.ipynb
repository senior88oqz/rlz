{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import nltk\n",
    "import re\n",
    "import sys\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kB = 1024\n",
    "mB = 1024 * kB\n",
    "collection = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_chunks(file_object, chunk_size=1024):\n",
    "    while True:\n",
    "        data = file_object.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1398432it [00:05, 260335.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "news = []\n",
    "with open('./data/news-week-aug24.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in tqdm(readCSV):\n",
    "        news.append(row[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398431/1398431 [02:52<00:00, 8116.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "wc = Counter()\n",
    "for head in tqdm(news[1:]):\n",
    "    tempt = word_tokenize(head)\n",
    "    for word in tempt:\n",
    "        wc[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('news.pickle', 'wb') as handle:\n",
    "    pickle.dump(wc, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08158413897225916\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for key,value in wc.items():\n",
    "    count += value\n",
    "print(len(wc.keys())/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1186496"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = set()\n",
    "# wc = 0\n",
    "# with open('./data/WestburyLab.Wikipedia.Corpus.txt') as file:\n",
    "#     for chunk in tqdm(read_in_chunks(file, chunk_size=10*mB)):\n",
    "#         tempt = word_tokenize(chunk)\n",
    "#         wc += len(tempt)\n",
    "#         vocab = vocab.union(set(tempt))\n",
    "#     print(wc, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# all_words = []\n",
    "# with open('./data/WestburyLab.Wikipedia.Corpus.txt') as file:\n",
    "#     for chunk in tqdm(read_in_chunks(file, chunk_size=100*mB)):\n",
    "#         tempt = word_tokenize(chunk)\n",
    "#         all_words.append(tempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL = []\n",
    "for chunk in tqdm(all_words):\n",
    "    for word in chunk:\n",
    "        ALL.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC = Counter(ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "labels, values = zip(*wc.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(labels,values)\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "plt.title(\"News Headline Word Distribution\")\n",
    "plt.xlabel(\"Vocabulary\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig('newsHeadline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('newsHeadline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./wiki.txt\", \"w\") as myFile:\n",
    "    for word in vocab:\n",
    "        myFile.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(collection[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a collection of 200MB    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "189it [00:00, 260.10it/s]\n"
     ]
    }
   ],
   "source": [
    "kB = 1024\n",
    "mB = 1024 * kB\n",
    "collection = {}\n",
    "with open('data/WestburyLab.Wikipedia.Corpus.txt','r') as file:\n",
    "    index = 0\n",
    "    for piece in tqdm(read_in_chunks(file, chunk_size=1*mB)):\n",
    "        collection[index] = piece\n",
    "        index += 1\n",
    "        if index == 200:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dictionary by regular sampling of size 1kB from first 100MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = \"\"\n",
    "for i in range(100):\n",
    "    dictionary += collection[i][:1*kB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match(a=0, b=15, size=9)\n",
      "apple pie\n",
      "apple pie\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "string1 = \"apple pie available\"\n",
    "string2 = \"come have some apple pies apple pies\"\n",
    "\n",
    "match = SequenceMatcher(None, string1, string2).find_longest_match(0, len(string1), 0, len(string2))\n",
    "\n",
    "print(match)  # -> Match(a=0, b=15, size=9)\n",
    "print(string1[match.a: match.a + match.size])  # -> apple pie\n",
    "print(string2[match.b: match.b + match.size])  # -> apple pie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrcut suffix array in $O(n\\log^2{n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, log2\n",
    "import numpy as np\n",
    "\n",
    "# https://codereview.stackexchange.com/questions/87335/suffix-array-construction-in-on-log2-n\n",
    "def sufar_np(txt):\n",
    "    \"\"\"\n",
    "    This implements the algorithm of Vladu and Negruşeri; \n",
    "    see http://web.stanford.edu/class/cs97si/suffix-array.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    if not txt:\n",
    "        return []\n",
    "    txt += chr(0)\n",
    "\n",
    "    equivalence = {t: i for i, t in enumerate(sorted(set(txt)))}\n",
    "    cls = np.array([equivalence[t] for t in txt])\n",
    "    ns = 2**np.arange(ceil(log2(len(txt))))\n",
    "\n",
    "    for n in ns[:-1]:\n",
    "        cls1 = np.roll(cls, -n)\n",
    "        inds = np.lexsort((cls1, cls))\n",
    "        result = np.logical_or(np.diff(cls[inds]), \n",
    "                               np.diff(cls1[inds]))\n",
    "\n",
    "        cls[inds[0]] = 0\n",
    "        cls[inds[1:]] = np.cumsum(result)\n",
    "\n",
    "    cls1 = np.roll(cls, ns[-1])\n",
    "    return np.lexsort((cls1, cls))[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.08 ms, sys: 1.01 ms, total: 2.09 ms\n",
      "Wall time: 7.19 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8, 4, 5, 1, 7, 3, 6, 2, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sufar_np('cabbaabba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor(i, target, dictionary, sufix_d):\n",
    "    \n",
    "    i, lb = 0, 0\n",
    "    rb = len(sufix_d) - 1\n",
    "    found = False\n",
    "    \n",
    "    p = len(sufix_d)/2\n",
    "    while lb <= rb and not found:\n",
    "        mid = (lb+rb)//2\n",
    "        t = target[:i+1]\n",
    "        sub =  dictionary[sufix_d[mid]][:i+1]\n",
    "        if t>sub:\n",
    "            rb = mid\n",
    "        elif t < sub:\n",
    "            lb = mid\n",
    "        # ??\n",
    "        else:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(157, 164)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m.span() for m in re.finditer('anarchy', dictionary)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_match(target, dictionary):\n",
    "    curr = 0\n",
    "    output = []\n",
    "    target_len = len(target)\n",
    "    while curr < target_len:\n",
    "        i = 0\n",
    "        go_on = True\n",
    "        while go_on: \n",
    "            match = [m.span() for m in re.finditer(re.escape(target[:i+1]), dictionary)]\n",
    "            longer_match = [m.span() for m in re.finditer(re.escape(target[:i+2]), dictionary)]\n",
    "            if len(longer_match) == 0:\n",
    "                if len(match) == 0:\n",
    "                    curr += 1\n",
    "                    output.append((target[i], 0))\n",
    "                else:\n",
    "                    length = match[0][1]-match[0][0]\n",
    "                    curr += length\n",
    "                    output.append((match[0][0],length))\n",
    "                go_on = False\n",
    "                target = target[i+1:]\n",
    "            else:\n",
    "                if i == len(target) - 1:\n",
    "                    length = longer_match[0][1] - longer_match[0][0]\n",
    "                    output.append((longer_match[0][0], length))\n",
    "                    return output\n",
    "                else:\n",
    "                    i += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 ms, sys: 1.78 ms, total: 18.3 ms\n",
      "Wall time: 17.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(157, 7), (21, 1)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time longest_match('anarchy ', dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    encode = []\n",
    "    block_size = 1 * kB\n",
    "    j = 0\n",
    "    for i in range(100):\n",
    "#         encode.append(longest_match(collection[i][j*block_size:(j+1)*block_size],dictionary\n",
    "#         print(j)\n",
    "#         j += 1\n",
    "        encode.append(longest_match(collection[i],dictionary))\n",
    "\n",
    "    return encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s ('"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection[1][1940:1943]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import groupby\n",
    "# from operator import itemgetter\n",
    "# # https://stackoverflow.com/questions/13560037/effcient-way-to-find-longest-duplicate-string-for-python-from-programming-pearl/13693834#\n",
    "# def longest_common_substring(text):\n",
    "#     \"\"\"Get the longest common substrings and their positions.\n",
    "#     >>> longest_common_substring('banana')\n",
    "#     {'ana': [1, 3]}\n",
    "#     >>> text = \"not so Agamemnon, who spoke fiercely to \"\n",
    "#     >>> sorted(longest_common_substring(text).items())\n",
    "#     [(' s', [3, 21]), ('no', [0, 13]), ('o ', [5, 20, 38])]\n",
    "\n",
    "#     This function can be easy modified for any criteria, e.g. for searching ten\n",
    "#     longest non overlapping repeated substrings.\n",
    "#     \"\"\"\n",
    "#     sa, rsa, lcp = suffix_array(text)\n",
    "#     maxlen = max(lcp)\n",
    "#     result = {}\n",
    "#     for i in range(1, len(text)):\n",
    "#         if lcp[i] == maxlen:\n",
    "#             j1, j2, h = sa[i - 1], sa[i], lcp[i]\n",
    "#             assert text[j1:j1 + h] == text[j2:j2 + h]\n",
    "#             substring = text[j1:j1 + h]\n",
    "#             if not substring in result:\n",
    "#                 result[substring] = [j1]\n",
    "#             result[substring].append(j2)\n",
    "#     return dict((k, sorted(v)) for k, v in result.items())\n",
    "\n",
    "# def suffix_array(text, _step=16):\n",
    "#     \"\"\"Analyze all common strings in the text.\n",
    "\n",
    "#     Short substrings of the length _step a are first pre-sorted. The are the \n",
    "#     results repeatedly merged so that the garanteed number of compared\n",
    "#     characters bytes is doubled in every iteration until all substrings are\n",
    "#     sorted exactly.\n",
    "\n",
    "#     Arguments:\n",
    "#         text:  The text to be analyzed.\n",
    "#         _step: Is only for optimization and testing. It is the optimal length\n",
    "#                of substrings used for initial pre-sorting. The bigger value is\n",
    "#                faster if there is enough memory. Memory requirements are\n",
    "#                approximately (estimate for 32 bit Python 3.3):\n",
    "#                    len(text) * (29 + (_size + 20 if _size > 2 else 0)) + 1MB\n",
    "\n",
    "#     Return value:      (tuple)\n",
    "#       (sa, rsa, lcp)\n",
    "#         sa:  Suffix array                  for i in range(1, size):\n",
    "#                assert text[sa[i-1]:] < text[sa[i]:]\n",
    "#         rsa: Reverse suffix array          for i in range(size):\n",
    "#                assert rsa[sa[i]] == i\n",
    "#         lcp: Longest common prefix         for i in range(1, size):\n",
    "#                assert text[sa[i-1]:sa[i-1]+lcp[i]] == text[sa[i]:sa[i]+lcp[i]]\n",
    "#                if sa[i-1] + lcp[i] < len(text):\n",
    "#                    assert text[sa[i-1] + lcp[i]] < text[sa[i] + lcp[i]]\n",
    "#     >>> suffix_array(text='banana')\n",
    "#     ([5, 3, 1, 0, 4, 2], [3, 2, 5, 1, 4, 0], [0, 1, 3, 0, 0, 2])\n",
    "\n",
    "#     Explanation: 'a' < 'ana' < 'anana' < 'banana' < 'na' < 'nana'\n",
    "#     The Longest Common String is 'ana': lcp[2] == 3 == len('ana')\n",
    "#     It is between  tx[sa[1]:] == 'ana' < 'anana' == tx[sa[2]:]\n",
    "#     \"\"\"\n",
    "#     tx = text\n",
    "#     size = len(tx)\n",
    "#     step = min(max(_step, 1), len(tx))\n",
    "#     sa = list(range(len(tx)))\n",
    "#     sa.sort(key=lambda i: tx[i:i + step])\n",
    "#     grpstart = size * [False] + [True]  # a boolean map for iteration speedup.\n",
    "#     # It helps to skip yet resolved values. The last value True is a sentinel.\n",
    "#     rsa = size * [None]\n",
    "#     stgrp, igrp = '', 0\n",
    "#     for i, pos in enumerate(sa):\n",
    "#         st = tx[pos:pos + step]\n",
    "#         if st != stgrp:\n",
    "#             grpstart[igrp] = (igrp < i - 1)\n",
    "#             stgrp = st\n",
    "#             igrp = i\n",
    "#         rsa[pos] = igrp\n",
    "#         sa[i] = pos\n",
    "#     grpstart[igrp] = (igrp < size - 1 or size == 0)\n",
    "#     while grpstart.index(True) < size:\n",
    "#         # assert step <= size\n",
    "#         nextgr = grpstart.index(True)\n",
    "#         while nextgr < size:\n",
    "#             igrp = nextgr\n",
    "#             nextgr = grpstart.index(True, igrp + 1)\n",
    "#             glist = []\n",
    "#             for ig in range(igrp, nextgr):\n",
    "#                 pos = sa[ig]\n",
    "#                 if rsa[pos] != igrp:\n",
    "#                     break\n",
    "#                 newgr = rsa[pos + step] if pos + step < size else -1\n",
    "#                 glist.append((newgr, pos))\n",
    "#             glist.sort()\n",
    "#             for ig, g in groupby(glist, key=itemgetter(0)):\n",
    "#                 g = [x[1] for x in g]\n",
    "#                 sa[igrp:igrp + len(g)] = g\n",
    "#                 grpstart[igrp] = (len(g) > 1)\n",
    "#                 for pos in g:\n",
    "#                     rsa[pos] = igrp\n",
    "#                 igrp += len(g)\n",
    "#         step *= 2\n",
    "#     del grpstart\n",
    "#     # create LCP array\n",
    "#     lcp = size * [None]\n",
    "#     h = 0\n",
    "#     for i in range(size):\n",
    "#         if rsa[i] > 0:\n",
    "#             j = sa[rsa[i] - 1]\n",
    "#             while i != size - h and j != size - h and tx[i + h] == tx[j + h]:\n",
    "#                 h += 1\n",
    "#             lcp[rsa[i]] = h\n",
    "#             if h > 0:\n",
    "#                 h -= 1\n",
    "#     if size > 0:\n",
    "#         lcp[0] = 0\n",
    "#     return sa, rsa, lcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
